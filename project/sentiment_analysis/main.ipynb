{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import project1 as p1\n",
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Data loading. There is no need to edit code in this section.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "train_data = utils.load_data('reviews_train.tsv')\n",
    "val_data = utils.load_data('reviews_val.tsv')\n",
    "test_data = utils.load_data('reviews_test.tsv')\n",
    "\n",
    "train_texts, train_labels = zip(*((sample['text'], sample['sentiment']) for sample in train_data))\n",
    "val_texts, val_labels = zip(*((sample['text'], sample['sentiment']) for sample in val_data))\n",
    "test_texts, test_labels = zip(*((sample['text'], sample['sentiment']) for sample in test_data))\n",
    "\n",
    "dictionary = p1.bag_of_words(train_texts)\n",
    "\n",
    "train_bow_features = p1.extract_bow_feature_vectors(train_texts, dictionary)\n",
    "val_bow_features = p1.extract_bow_feature_vectors(val_texts, dictionary)\n",
    "test_bow_features = p1.extract_bow_feature_vectors(test_texts, dictionary)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Problem 5\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "toy_features, toy_labels = toy_data = utils.load_toy_data('toy_data.tsv')\n",
    "\n",
    "T = 100\n",
    "L = 0.2\n",
    "\n",
    "thetas_perceptron = p1.perceptron(toy_features, toy_labels, T)\n",
    "thetas_avg_perceptron = p1.average_perceptron(toy_features, toy_labels, T)\n",
    "thetas_pegasos = p1.pegasos(toy_features, toy_labels, T, L)\n",
    "\n",
    "def plot_toy_results(algo_name, thetas):\n",
    "    print('theta for', algo_name, 'is', ', '.join(map(str,list(thetas[0]))))\n",
    "    print('theta_0 for', algo_name, 'is', str(thetas[1]))\n",
    "    utils.plot_toy_data(algo_name, toy_features, toy_labels, thetas)\n",
    "\n",
    "plot_toy_results('Perceptron', thetas_perceptron)\n",
    "plot_toy_results('Average Perceptron', thetas_avg_perceptron)\n",
    "plot_toy_results('Pegasos', thetas_pegasos)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Problem 7\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# T = 10\n",
    "# L = 0.01\n",
    "#\n",
    "# pct_train_accuracy, pct_val_accuracy = \\\n",
    "#    p1.classifier_accuracy(p1.perceptron, train_bow_features,val_bow_features,train_labels,val_labels,T=T)\n",
    "# print(\"{:35} {:.4f}\".format(\"Training accuracy for perceptron:\", pct_train_accuracy))\n",
    "# print(\"{:35} {:.4f}\".format(\"Validation accuracy for perceptron:\", pct_val_accuracy))\n",
    "#\n",
    "# avg_pct_train_accuracy, avg_pct_val_accuracy = \\\n",
    "#    p1.classifier_accuracy(p1.average_perceptron, train_bow_features,val_bow_features,train_labels,val_labels,T=T)\n",
    "# print(\"{:43} {:.4f}\".format(\"Training accuracy for average perceptron:\", avg_pct_train_accuracy))\n",
    "# print(\"{:43} {:.4f}\".format(\"Validation accuracy for average perceptron:\", avg_pct_val_accuracy))\n",
    "#\n",
    "# avg_peg_train_accuracy, avg_peg_val_accuracy = \\\n",
    "#    p1.classifier_accuracy(p1.pegasos, train_bow_features,val_bow_features,train_labels,val_labels,T=T,L=L)\n",
    "# print(\"{:50} {:.4f}\".format(\"Training accuracy for Pegasos:\", avg_peg_train_accuracy))\n",
    "# print(\"{:50} {:.4f}\".format(\"Validation accuracy for Pegasos:\", avg_peg_val_accuracy))\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Problem 8\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# data = (train_bow_features, train_labels, val_bow_features, val_labels)\n",
    "#\n",
    "# # values of T and lambda to try\n",
    "# Ts = [1, 5, 10, 15, 25, 50]\n",
    "# Ls = [0.001, 0.01, 0.1, 1, 10]\n",
    "#\n",
    "# pct_tune_results = utils.tune_perceptron(Ts, *data)\n",
    "# print('perceptron valid:', list(zip(Ts, pct_tune_results[1])))\n",
    "# print('best = {:.4f}, T={:.4f}'.format(np.max(pct_tune_results[1]), Ts[np.argmax(pct_tune_results[1])]))\n",
    "#\n",
    "# avg_pct_tune_results = utils.tune_avg_perceptron(Ts, *data)\n",
    "# print('avg perceptron valid:', list(zip(Ts, avg_pct_tune_results[1])))\n",
    "# print('best = {:.4f}, T={:.4f}'.format(np.max(avg_pct_tune_results[1]), Ts[np.argmax(avg_pct_tune_results[1])]))\n",
    "#\n",
    "# # fix values for L and T while tuning Pegasos T and L, respective\n",
    "# fix_L = 0.01\n",
    "# peg_tune_results_T = utils.tune_pegasos_T(fix_L, Ts, *data)\n",
    "# print('Pegasos valid: tune T', list(zip(Ts, peg_tune_results_T[1])))\n",
    "# print('best = {:.4f}, T={:.4f}'.format(np.max(peg_tune_results_T[1]), Ts[np.argmax(peg_tune_results_T[1])]))\n",
    "#\n",
    "# fix_T = Ts[np.argmax(peg_tune_results_T[1])]\n",
    "# peg_tune_results_L = utils.tune_pegasos_L(fix_T, Ls, *data)\n",
    "# print('Pegasos valid: tune L', list(zip(Ls, peg_tune_results_L[1])))\n",
    "# print('best = {:.4f}, L={:.4f}'.format(np.max(peg_tune_results_L[1]), Ls[np.argmax(peg_tune_results_L[1])]))\n",
    "#\n",
    "# utils.plot_tune_results('Perceptron', 'T', Ts, *pct_tune_results)\n",
    "# utils.plot_tune_results('Avg Perceptron', 'T', Ts, *avg_pct_tune_results)\n",
    "# utils.plot_tune_results('Pegasos', 'T', Ts, *peg_tune_results_T)\n",
    "# utils.plot_tune_results('Pegasos', 'L', Ls, *peg_tune_results_L)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Use the best method (perceptron, average perceptron or Pegasos) along with\n",
    "# the optimal hyperparameters according to validation accuracies to test\n",
    "# against the test dataset. The test data has been provided as\n",
    "# test_bow_features and test_labels.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Your code here\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Assign to best_theta, the weights (and not the bias!) learned by your most\n",
    "# accurate algorithm with the optimal choice of hyperparameters.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# best_theta = None # Your code here\n",
    "# wordlist   = [word for (idx, word) in sorted(zip(dictionary.values(), dictionary.keys()))]\n",
    "# sorted_word_features = utils.most_explanatory_word(best_theta, wordlist)\n",
    "# print(\"Most Explanatory Word Features\")\n",
    "# print(sorted_word_features[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import project1 as p1\n",
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Data loading. There is no need to edit code in this section.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "train_data = utils.load_data('reviews_train.tsv')\n",
    "val_data = utils.load_data('reviews_val.tsv')\n",
    "test_data = utils.load_data('reviews_test.tsv')\n",
    "\n",
    "train_texts, train_labels = zip(*((sample['text'], sample['sentiment']) for sample in train_data))\n",
    "val_texts, val_labels = zip(*((sample['text'], sample['sentiment']) for sample in val_data))\n",
    "test_texts, test_labels = zip(*((sample['text'], sample['sentiment']) for sample in test_data))\n",
    "\n",
    "dictionary = p1.bag_of_words(train_texts)\n",
    "\n",
    "train_bow_features = p1.extract_bow_feature_vectors(train_texts, dictionary)\n",
    "val_bow_features = p1.extract_bow_feature_vectors(val_texts, dictionary)\n",
    "test_bow_features = p1.extract_bow_feature_vectors(test_texts, dictionary)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Problem 5\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# toy_features, toy_labels = toy_data = utils.load_toy_data('toy_data.tsv')\n",
    "\n",
    "# T = 50\n",
    "# L = 0.2\n",
    "\n",
    "# thetas_perceptron = p1.perceptron(toy_features, toy_labels, T)\n",
    "# thetas_avg_perceptron = p1.average_perceptron(toy_features, toy_labels, T)\n",
    "# thetas_pegasos = p1.pegasos(toy_features, toy_labels, T, L)\n",
    "\n",
    "# def plot_toy_results(algo_name, thetas):\n",
    "#     print('theta for', algo_name, 'is', ', '.join(map(str,list(thetas[0]))))\n",
    "#     print('theta_0 for', algo_name, 'is', str(thetas[1]))\n",
    "#     utils.plot_toy_data(algo_name, toy_features, toy_labels, thetas)\n",
    "\n",
    "# plot_toy_results('Perceptron', thetas_perceptron)\n",
    "# plot_toy_results('Average Perceptron', thetas_avg_perceptron)\n",
    "# plot_toy_results('Pegasos', thetas_pegasos)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Problem 7\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "T = 10\n",
    "L = 0.01\n",
    "\n",
    "pct_train_accuracy, pct_val_accuracy = \\\n",
    "   p1.classifier_accuracy(p1.perceptron, train_bow_features,val_bow_features,train_labels,val_labels,T=T)\n",
    "print(\"{:35} {:.4f}\".format(\"Training accuracy for perceptron:\", pct_train_accuracy))\n",
    "print(\"{:35} {:.4f}\".format(\"Validation accuracy for perceptron:\", pct_val_accuracy))\n",
    "\n",
    "avg_pct_train_accuracy, avg_pct_val_accuracy = \\\n",
    "   p1.classifier_accuracy(p1.average_perceptron, train_bow_features,val_bow_features,train_labels,val_labels,T=T)\n",
    "print(\"{:43} {:.4f}\".format(\"Training accuracy for average perceptron:\", avg_pct_train_accuracy))\n",
    "print(\"{:43} {:.4f}\".format(\"Validation accuracy for average perceptron:\", avg_pct_val_accuracy))\n",
    "\n",
    "avg_peg_train_accuracy, avg_peg_val_accuracy = \\\n",
    "   p1.classifier_accuracy(p1.pegasos, train_bow_features,val_bow_features,train_labels,val_labels,T=T,L=L)\n",
    "print(\"{:50} {:.4f}\".format(\"Training accuracy for Pegasos:\", avg_peg_train_accuracy))\n",
    "print(\"{:50} {:.4f}\".format(\"Validation accuracy for Pegasos:\", avg_peg_val_accuracy))\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Problem 8\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# data = (train_bow_features, train_labels, val_bow_features, val_labels)\n",
    "#\n",
    "# # values of T and lambda to try\n",
    "# Ts = [1, 5, 10, 15, 25, 50]\n",
    "# Ls = [0.001, 0.01, 0.1, 1, 10]\n",
    "#\n",
    "# pct_tune_results = utils.tune_perceptron(Ts, *data)\n",
    "# print('perceptron valid:', list(zip(Ts, pct_tune_results[1])))\n",
    "# print('best = {:.4f}, T={:.4f}'.format(np.max(pct_tune_results[1]), Ts[np.argmax(pct_tune_results[1])]))\n",
    "#\n",
    "# avg_pct_tune_results = utils.tune_avg_perceptron(Ts, *data)\n",
    "# print('avg perceptron valid:', list(zip(Ts, avg_pct_tune_results[1])))\n",
    "# print('best = {:.4f}, T={:.4f}'.format(np.max(avg_pct_tune_results[1]), Ts[np.argmax(avg_pct_tune_results[1])]))\n",
    "#\n",
    "# # fix values for L and T while tuning Pegasos T and L, respective\n",
    "# fix_L = 0.01\n",
    "# peg_tune_results_T = utils.tune_pegasos_T(fix_L, Ts, *data)\n",
    "# print('Pegasos valid: tune T', list(zip(Ts, peg_tune_results_T[1])))\n",
    "# print('best = {:.4f}, T={:.4f}'.format(np.max(peg_tune_results_T[1]), Ts[np.argmax(peg_tune_results_T[1])]))\n",
    "#\n",
    "# fix_T = Ts[np.argmax(peg_tune_results_T[1])]\n",
    "# peg_tune_results_L = utils.tune_pegasos_L(fix_T, Ls, *data)\n",
    "# print('Pegasos valid: tune L', list(zip(Ls, peg_tune_results_L[1])))\n",
    "# print('best = {:.4f}, L={:.4f}'.format(np.max(peg_tune_results_L[1]), Ls[np.argmax(peg_tune_results_L[1])]))\n",
    "#\n",
    "# utils.plot_tune_results('Perceptron', 'T', Ts, *pct_tune_results)\n",
    "# utils.plot_tune_results('Avg Perceptron', 'T', Ts, *avg_pct_tune_results)\n",
    "# utils.plot_tune_results('Pegasos', 'T', Ts, *peg_tune_results_T)\n",
    "# utils.plot_tune_results('Pegasos', 'L', Ls, *peg_tune_results_L)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Use the best method (perceptron, average perceptron or Pegasos) along with\n",
    "# the optimal hyperparameters according to validation accuracies to test\n",
    "# against the test dataset. The test data has been provided as\n",
    "# test_bow_features and test_labels.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Your code here\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Assign to best_theta, the weights (and not the bias!) learned by your most\n",
    "# accurate algorithm with the optimal choice of hyperparameters.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# best_theta = None # Your code here\n",
    "# wordlist   = [word for (idx, word) in sorted(zip(dictionary.values(), dictionary.keys()))]\n",
    "# sorted_word_features = utils.most_explanatory_word(best_theta, wordlist)\n",
    "# print(\"Most Explanatory Word Features\")\n",
    "# print(sorted_word_features[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/MITx-6.86x-ML/project/sentiment_analysis/main.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borganic-space-enigma-w95rp9jvgw93v6wq/workspaces/MITx-6.86x-ML/project/sentiment_analysis/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m test_bow_features \u001b[39m=\u001b[39m p1\u001b[39m.\u001b[39mextract_bow_feature_vectors(test_texts, dictionary)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borganic-space-enigma-w95rp9jvgw93v6wq/workspaces/MITx-6.86x-ML/project/sentiment_analysis/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# -------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borganic-space-enigma-w95rp9jvgw93v6wq/workspaces/MITx-6.86x-ML/project/sentiment_analysis/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Problem 5\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borganic-space-enigma-w95rp9jvgw93v6wq/workspaces/MITx-6.86x-ML/project/sentiment_analysis/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# -------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Borganic-space-enigma-w95rp9jvgw93v6wq/workspaces/MITx-6.86x-ML/project/sentiment_analysis/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=120'>121</a>\u001b[0m \u001b[39m# print(\"Most Explanatory Word Features\")\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Borganic-space-enigma-w95rp9jvgw93v6wq/workspaces/MITx-6.86x-ML/project/sentiment_analysis/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=121'>122</a>\u001b[0m \u001b[39m# print(sorted_word_features[:10])\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://codespaces%2Borganic-space-enigma-w95rp9jvgw93v6wq/workspaces/MITx-6.86x-ML/project/sentiment_analysis/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=123'>124</a>\u001b[0m train_acc, val_acc \u001b[39m=\u001b[39m p1\u001b[39m.\u001b[39;49mclassifier_accuracy(\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Borganic-space-enigma-w95rp9jvgw93v6wq/workspaces/MITx-6.86x-ML/project/sentiment_analysis/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=124'>125</a>\u001b[0m     classifier\u001b[39m=\u001b[39;49mp1\u001b[39m.\u001b[39;49mpegasos, train_feature_matrix\u001b[39m=\u001b[39;49mtrain_bow_features,\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Borganic-space-enigma-w95rp9jvgw93v6wq/workspaces/MITx-6.86x-ML/project/sentiment_analysis/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=125'>126</a>\u001b[0m     val_feature_matrix\u001b[39m=\u001b[39;49mtrain_bow_features,train_labels\u001b[39m=\u001b[39;49mtrain_labels, \n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Borganic-space-enigma-w95rp9jvgw93v6wq/workspaces/MITx-6.86x-ML/project/sentiment_analysis/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=126'>127</a>\u001b[0m      val_labels\u001b[39m=\u001b[39;49mval_labels, T\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m, L\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Borganic-space-enigma-w95rp9jvgw93v6wq/workspaces/MITx-6.86x-ML/project/sentiment_analysis/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=128'>129</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m\"\u001b[39m, train_acc)\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Borganic-space-enigma-w95rp9jvgw93v6wq/workspaces/MITx-6.86x-ML/project/sentiment_analysis/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=129'>130</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mValidation\u001b[39m\u001b[39m\"\u001b[39m, val_acc)\n",
      "File \u001b[0;32m/workspaces/MITx-6.86x-ML/project/sentiment_analysis/project1.py:355\u001b[0m, in \u001b[0;36mclassifier_accuracy\u001b[0;34m(classifier, train_feature_matrix, val_feature_matrix, train_labels, val_labels, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m result_train_labels\u001b[39m=\u001b[39mclassify(train_feature_matrix,theta,theta_0)\n\u001b[1;32m    354\u001b[0m result_val_labels\u001b[39m=\u001b[39mclassify(val_feature_matrix,theta,theta_0)\n\u001b[0;32m--> 355\u001b[0m accuracy_res_val\u001b[39m=\u001b[39maccuracy(result_val_labels,val_labels)\n\u001b[1;32m    356\u001b[0m accuracy_res_train\u001b[39m=\u001b[39maccuracy(result_train_labels,train_labels)\n\u001b[1;32m    357\u001b[0m \u001b[39mreturn\u001b[39;00m (accuracy_res_train,accuracy_res_val)\n",
      "File \u001b[0;32m/workspaces/MITx-6.86x-ML/project/sentiment_analysis/project1.py:449\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(preds, targets)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maccuracy\u001b[39m(preds, targets):\n\u001b[1;32m    445\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[39m    Given length-N vectors containing predicted and target labels,\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39m    returns the fraction of predictions that are correct.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m (preds \u001b[39m==\u001b[39;49m targets)\u001b[39m.\u001b[39;49mmean()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "import project1 as p1\n",
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Data loading. There is no need to edit code in this section.\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "train_data = utils.load_data('reviews_train.tsv')\n",
    "val_data = utils.load_data('reviews_val.tsv')\n",
    "test_data = utils.load_data('reviews_test.tsv')\n",
    "\n",
    "train_texts, train_labels = zip(\n",
    "    *((sample['text'], sample['sentiment']) for sample in train_data))\n",
    "val_texts, val_labels = zip(\n",
    "    *((sample['text'], sample['sentiment']) for sample in val_data))\n",
    "test_texts, test_labels = zip(\n",
    "    *((sample['text'], sample['sentiment']) for sample in test_data))\n",
    "\n",
    "dictionary = p1.bag_of_words(train_texts)\n",
    "\n",
    "train_bow_features = p1.extract_bow_feature_vectors(train_texts, dictionary)\n",
    "val_bow_features = p1.extract_bow_feature_vectors(val_texts, dictionary)\n",
    "test_bow_features = p1.extract_bow_feature_vectors(test_texts, dictionary)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Problem 5\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "# toy_features, toy_labels = toy_data = utils.load_toy_data('toy_data.tsv')\n",
    "\n",
    "# T = 50\n",
    "# L = 0.2\n",
    "\n",
    "# thetas_perceptron = p1.perceptron(toy_features, toy_labels, T)\n",
    "# thetas_avg_perceptron = p1.average_perceptron(toy_features, toy_labels, T)\n",
    "# thetas_pegasos = p1.pegasos(toy_features, toy_labels, T, L)\n",
    "\n",
    "# def plot_toy_results(algo_name, thetas):\n",
    "#     print('theta for', algo_name, 'is', ', '.join(map(str,list(thetas[0]))))\n",
    "#     print('theta_0 for', algo_name, 'is', str(thetas[1]))\n",
    "#     utils.plot_toy_data(algo_name, toy_features, toy_labels, thetas)\n",
    "\n",
    "# plot_toy_results('Perceptron', thetas_perceptron)\n",
    "# plot_toy_results('Average Perceptron', thetas_avg_perceptron)\n",
    "# plot_toy_results('Pegasos', thetas_pegasos)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Problem 7\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "# T = 10\n",
    "# L = 0.01\n",
    "\n",
    "# pct_train_accuracy, pct_val_accuracy = \\\n",
    "#    p1.classifier_accuracy(p1.perceptron, train_bow_features,val_bow_features,train_labels,val_labels,T=T)\n",
    "# print(\"{:35} {:.4f}\".format(\"Training accuracy for perceptron:\", pct_train_accuracy))\n",
    "# print(\"{:35} {:.4f}\".format(\"Validation accuracy for perceptron:\", pct_val_accuracy))\n",
    "\n",
    "# avg_pct_train_accuracy, avg_pct_val_accuracy = \\\n",
    "#    p1.classifier_accuracy(p1.average_perceptron, train_bow_features,val_bow_features,train_labels,val_labels,T=T)\n",
    "# print(\"{:43} {:.4f}\".format(\"Training accuracy for average perceptron:\", avg_pct_train_accuracy))\n",
    "# print(\"{:43} {:.4f}\".format(\"Validation accuracy for average perceptron:\", avg_pct_val_accuracy))\n",
    "\n",
    "# avg_peg_train_accuracy, avg_peg_val_accuracy = \\\n",
    "#    p1.classifier_accuracy(p1.pegasos, train_bow_features,val_bow_features,train_labels,val_labels,T=T,L=L)\n",
    "# print(\"{:50} {:.4f}\".format(\"Training accuracy for Pegasos:\", avg_peg_train_accuracy))\n",
    "# print(\"{:50} {:.4f}\".format(\"Validation accuracy for Pegasos:\", avg_peg_val_accuracy))\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Problem 8\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "# data = (train_bow_features, train_labels, val_bow_features, val_labels)\n",
    "\n",
    "# # values of T and lambda to try\n",
    "# Ts = [1, 5, 10, 15, 25, 50]\n",
    "# Ls = [0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "# pct_tune_results = utils.tune_perceptron(Ts, *data)\n",
    "# print('perceptron valid:', list(zip(Ts, pct_tune_results[1])))\n",
    "# print('best = {:.4f}, T={:.4f}'.format(np.max(pct_tune_results[1]), Ts[np.argmax(pct_tune_results[1])]))\n",
    "\n",
    "# avg_pct_tune_results = utils.tune_avg_perceptron(Ts, *data)\n",
    "# print('avg perceptron valid:', list(zip(Ts, avg_pct_tune_results[1])))\n",
    "# print('best = {:.4f}, T={:.4f}'.format(np.max(avg_pct_tune_results[1]), Ts[np.argmax(avg_pct_tune_results[1])]))\n",
    "\n",
    "# # fix values for L and T while tuning Pegasos T and L, respective\n",
    "# fix_L = 0.01\n",
    "# peg_tune_results_T = utils.tune_pegasos_T(fix_L, Ts, *data)\n",
    "# print('Pegasos valid: tune T', list(zip(Ts, peg_tune_results_T[1])))\n",
    "# print('best = {:.4f}, T={:.4f}'.format(np.max(peg_tune_results_T[1]), Ts[np.argmax(peg_tune_results_T[1])]))\n",
    "\n",
    "# fix_T = Ts[np.argmax(peg_tune_results_T[1])]\n",
    "# peg_tune_results_L = utils.tune_pegasos_L(fix_T, Ls, *data)\n",
    "# print('Pegasos valid: tune L', list(zip(Ls, peg_tune_results_L[1])))\n",
    "# print('best = {:.4f}, L={:.4f}'.format(np.max(peg_tune_results_L[1]), Ls[np.argmax(peg_tune_results_L[1])]))\n",
    "\n",
    "# utils.plot_tune_results('Perceptron', 'T', Ts, *pct_tune_results)\n",
    "# utils.plot_tune_results('Avg Perceptron', 'T', Ts, *avg_pct_tune_results)\n",
    "# utils.plot_tune_results('Pegasos', 'T', Ts, *peg_tune_results_T)\n",
    "# utils.plot_tune_results('Pegasos', 'L', Ls, *peg_tune_results_L)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Use the best method (perceptron, average perceptron or Pegasos) along with\n",
    "# the optimal hyperparameters according to validation accuracies to test\n",
    "# against the test dataset. The test data has been provided as\n",
    "# test_bow_features and test_labels.\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# Assign to best_theta, the weights (and not the bias!) learned by your most\n",
    "# accurate algorithm with the optimal choice of hyperparameters.\n",
    "# -------------------------------------------------------------------------------\n",
    "# print(\"--------------------------------------------------------------------------\")\n",
    "# best_theta,best_theta_0 = p1.pegasos(feature_matrix=train_bow_features,labels=train_labels,L=0.01,T=25) # Your code here\n",
    "# wordlist   = [word for (idx, word) in sorted(zip(dictionary.values(), dictionary.keys()))]\n",
    "# sorted_word_features = utils.most_explanatory_word(best_theta, wordlist)\n",
    "# print(\"Most Explanatory Word Features\")\n",
    "# print(sorted_word_features[:10])\n",
    "\n",
    "train_acc, val_acc = p1.classifier_accuracy(\n",
    "    classifier=p1.pegasos, train_feature_matrix=train_bow_features,\n",
    "    val_feature_matrix=train_bow_features,train_labels=train_labels, \n",
    "     val_labels=val_labels, T=25, L=0.01)\n",
    "\n",
    "print(\"Train\", train_acc)\n",
    "print(\"Validation\", val_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6.86x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
